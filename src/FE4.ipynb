{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, warnings, random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "## ------------------- \n",
    "\n",
    "## -------------------\n",
    "## Memory Reducer\n",
    "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
    "# :verbose                                        # type: bool\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "LOCAL_TEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n"
     ]
    }
   ],
   "source": [
    "print('Load Data')\n",
    "train_df = pd.read_csv('train_transaction.csv')\n",
    "test_df = pd.read_csv('test_transaction.csv')\n",
    "test_df['isFraud'] = 0\n",
    "\n",
    "train_identity = pd.read_csv('train_identity.csv')\n",
    "test_identity = pd.read_csv('test_identity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCAL_TEST:\n",
    "    for df2 in [train_df, test_df, train_identity, test_identity]:\n",
    "        df = reduce_mem_usage(df2)\n",
    "\n",
    "        for col in list(df):\n",
    "            if not df[col].equals(df2[col]):\n",
    "                print('Bad transformation', col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 542.00 Mb (69.0% reduction)\n",
      "Mem. usage decreased to 473.00 Mb (68.0% reduction)\n",
      "Mem. usage decreased to 25.00 Mb (44.0% reduction)\n",
      "Mem. usage decreased to 25.00 Mb (43.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_df = reduce_mem_usage(train_df)\n",
    "test_df  = reduce_mem_usage(test_df)\n",
    "\n",
    "train_identity = reduce_mem_usage(train_identity)\n",
    "test_identity  = reduce_mem_usage(test_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Encoding', 'card4')\n",
      "{'american express': 16009, 'visa': 719649, 'discover': 9524, 'mastercard': 347386}\n",
      "('Encoding', 'card6')\n",
      "{'credit': 267648, 'debit or credit': 30, 'charge card': 16, 'debit': 824959}\n",
      "('Encoding', 'ProductCD')\n",
      "{'H': 62397, 'C': 137785, 'R': 73346, 'S': 23046, 'W': 800657}\n"
     ]
    }
   ],
   "source": [
    "for col in ['card4', 'card6', 'ProductCD']:\n",
    "    print('Encoding', col)\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    col_encoded = temp_df[col].value_counts().to_dict()   \n",
    "    train_df[col] = train_df[col].map(col_encoded)\n",
    "    test_df[col]  = test_df[col].map(col_encoded)\n",
    "    print(col_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Encoding', 'M4')\n",
      "{'M1': 97306, 'M0': 357789, 'M2': 122947}\n"
     ]
    }
   ],
   "source": [
    "for col in ['M1','M2','M3','M5','M6','M7','M8','M9']:\n",
    "    train_df[col] = train_df[col].map({'T':1, 'F':0})\n",
    "    test_df[col]  = test_df[col].map({'T':1, 'F':0})\n",
    "\n",
    "for col in ['M4']:\n",
    "    print('Encoding', col)\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    col_encoded = temp_df[col].value_counts().to_dict()   \n",
    "    train_df[col] = train_df[col].map(col_encoded)\n",
    "    test_df[col]  = test_df[col].map(col_encoded)\n",
    "    print(col_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minify_identity_df(df):\n",
    "\n",
    "    df['id_12'] = df['id_12'].map({'Found':1, 'NotFound':0})\n",
    "    df['id_15'] = df['id_15'].map({'New':2, 'Found':1, 'Unknown':0})\n",
    "    df['id_16'] = df['id_16'].map({'Found':1, 'NotFound':0})\n",
    "\n",
    "    df['id_23'] = df['id_23'].map({'TRANSPARENT':4, 'IP_PROXY':3, 'IP_PROXY:ANONYMOUS':2, 'IP_PROXY:HIDDEN':1})\n",
    "\n",
    "    df['id_27'] = df['id_27'].map({'Found':1, 'NotFound':0})\n",
    "    df['id_28'] = df['id_28'].map({'New':2, 'Found':1})\n",
    "\n",
    "    df['id_29'] = df['id_29'].map({'Found':1, 'NotFound':0})\n",
    "\n",
    "    df['id_35'] = df['id_35'].map({'T':1, 'F':0})\n",
    "    df['id_36'] = df['id_36'].map({'T':1, 'F':0})\n",
    "    df['id_37'] = df['id_37'].map({'T':1, 'F':0})\n",
    "    df['id_38'] = df['id_38'].map({'T':1, 'F':0})\n",
    "\n",
    "    df['id_34'] = df['id_34'].fillna(':0')\n",
    "    df['id_34'] = df['id_34'].apply(lambda x: x.split(':')[1]).astype(np.int8)\n",
    "    df['id_34'] = np.where(df['id_34']==0, np.nan, df['id_34'])\n",
    "    \n",
    "    df['id_33'] = df['id_33'].fillna('0x0')\n",
    "    df['id_33_0'] = df['id_33'].apply(lambda x: x.split('x')[0]).astype(int)\n",
    "    df['id_33_1'] = df['id_33'].apply(lambda x: x.split('x')[1]).astype(int)\n",
    "    df['id_33'] = np.where(df['id_33']=='0x0', np.nan, df['id_33'])\n",
    "\n",
    "    df['DeviceType'].map({'desktop':1, 'mobile':0})\n",
    "    return df\n",
    "\n",
    "train_identity = minify_identity_df(train_identity)\n",
    "test_identity = minify_identity_df(test_identity)\n",
    "\n",
    "for col in ['id_33']:\n",
    "    train_identity[col] = train_identity[col].fillna('unseen_before_label')\n",
    "    test_identity[col]  = test_identity[col].fillna('unseen_before_label')\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train_identity[col])+list(test_identity[col]))\n",
    "    train_identity[col] = le.transform(train_identity[col])\n",
    "    test_identity[col]  = le.transform(test_identity[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 506.00 Mb (6.0% reduction)\n",
      "Mem. usage decreased to 442.00 Mb (6.0% reduction)\n",
      "Mem. usage decreased to 15.00 Mb (46.0% reduction)\n",
      "Mem. usage decreased to 15.00 Mb (44.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_df = reduce_mem_usage(train_df)\n",
    "test_df  = reduce_mem_usage(test_df)\n",
    "\n",
    "train_identity = reduce_mem_usage(train_identity)\n",
    "test_identity  = reduce_mem_usage(test_identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle('train_transaction.pkl')\n",
    "test_df.to_pickle('test_transaction.pkl')\n",
    "\n",
    "train_identity.to_pickle('train_identity.pkl')\n",
    "test_identity.to_pickle('test_identity.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, warnings, random\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "LOCAL_TEST = False\n",
    "TARGET = 'isFraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n"
     ]
    }
   ],
   "source": [
    "print('Load Data')\n",
    "train_df = pd.read_pickle('train_transaction.pkl')\n",
    "\n",
    "if LOCAL_TEST:\n",
    "    test_df = train_df.iloc[-100000:,].reset_index(drop=True)\n",
    "    train_df = train_df.iloc[:400000,].reset_index(drop=True)\n",
    "    \n",
    "    train_identity = pd.read_pickle('train_identity.pkl')\n",
    "    test_identity  = train_identity[train_identity['TransactionID'].isin(test_df['TransactionID'])].reset_index(drop=True)\n",
    "    train_identity = train_identity[train_identity['TransactionID'].isin(train_df['TransactionID'])].reset_index(drop=True)\n",
    "else:\n",
    "    test_df = pd.read_pickle('test_transaction.pkl')\n",
    "    test_identity = pd.read_pickle('test_identity.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_card = train_df['card1'].value_counts()\n",
    "valid_card = valid_card[valid_card>10]\n",
    "valid_card = list(valid_card.index)\n",
    "    \n",
    "train_df['card1'] = np.where(train_df['card1'].isin(valid_card), train_df['card1'], np.nan)\n",
    "test_df['card1']  = np.where(test_df['card1'].isin(valid_card), test_df['card1'], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_cols = ['card1','card2','card3','card5',\n",
    "          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n",
    "          'D1','D2','D3','D4','D5','D6','D7','D8','D9',\n",
    "          'addr1','addr2',\n",
    "          'dist1','dist2',\n",
    "          'P_emaildomain', 'R_emaildomain'\n",
    "         ]\n",
    "\n",
    "for col in i_cols:\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    fq_encode = temp_df[col].value_counts().to_dict()   \n",
    "    train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n",
    "    test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['ProductCD','M4']:\n",
    "    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n",
    "                                                        columns={'mean': col+'_target_mean'})\n",
    "    temp_dict.index = temp_dict[col].values\n",
    "    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n",
    "\n",
    "    train_df[col+'_target_mean'] = train_df[col].map(temp_dict)\n",
    "    test_df[col+'_target_mean']  = test_df[col].map(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_emaildomain\n",
      "R_emaildomain\n"
     ]
    }
   ],
   "source": [
    "for col in list(train_df):\n",
    "    if train_df[col].dtype=='O':\n",
    "        print(col)\n",
    "        train_df[col] = train_df[col].fillna('unseen_before_label')\n",
    "        test_df[col]  = test_df[col].fillna('unseen_before_label')\n",
    "        \n",
    "        train_df[col] = train_df[col].astype(str)\n",
    "        test_df[col] = test_df[col].astype(str)\n",
    "        \n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_df[col])+list(test_df[col]))\n",
    "        train_df[col] = le.transform(train_df[col])\n",
    "        test_df[col]  = le.transform(test_df[col])\n",
    "        \n",
    "        train_df[col] = train_df[col].astype('category')\n",
    "        test_df[col] = test_df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['uid'] = train_df['card1'].astype(str)+'_'+train_df['card2'].astype(str)+'_'+train_df['card3'].astype(str)+'_'+train_df['card4'].astype(str)\n",
    "test_df['uid'] = test_df['card1'].astype(str)+'_'+test_df['card2'].astype(str)+'_'+test_df['card3'].astype(str)+'_'+test_df['card4'].astype(str)\n",
    "\n",
    "train_df['uid2'] = train_df['uid'].astype(str)+'_'+train_df['addr1'].astype(str)+'_'+train_df['addr2'].astype(str)\n",
    "test_df['uid2'] = test_df['uid'].astype(str)+'_'+test_df['addr1'].astype(str)+'_'+test_df['addr2'].astype(str)\n",
    "\n",
    "# Check if Transaction Amount is common or not (we can use freq encoding here)\n",
    "# In our dialog with model we are telling to trust or not to these values  \n",
    "valid_card = train_df['TransactionAmt'].value_counts()\n",
    "valid_card = valid_card[valid_card>10]\n",
    "valid_card = list(valid_card.index)\n",
    "    \n",
    "train_df['TransactionAmt_check'] = np.where(train_df['TransactionAmt'].isin(test_df['TransactionAmt']), 1, 0)\n",
    "test_df['TransactionAmt_check']  = np.where(test_df['TransactionAmt'].isin(train_df['TransactionAmt']), 1, 0)\n",
    "\n",
    "# For our model current TransactionAmt is a noise (even when features importances are telling contrariwise)\n",
    "# There are many unique values and model doesn't generalize well\n",
    "# Lets do some aggregations\n",
    "i_cols = ['card1','card2','card3','card5','uid','uid2']\n",
    "\n",
    "for col in i_cols:\n",
    "    for agg_type in ['mean', 'std']:\n",
    "        new_col_name = col+'_TransactionAmt_'+agg_type\n",
    "        temp_df = pd.concat([train_df[[col, 'TransactionAmt']], test_df[[col,'TransactionAmt']]])\n",
    "        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n",
    "                                                columns={agg_type: new_col_name})\n",
    "        \n",
    "        temp_df.index = list(temp_df[col])\n",
    "        temp_df = temp_df[new_col_name].to_dict()   \n",
    "    \n",
    "        train_df[new_col_name] = train_df[col].map(temp_df)\n",
    "        test_df[new_col_name]  = test_df[col].map(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['bank_type'] = train_df['card3'].astype(str)+'_'+train_df['card5'].astype(str)\n",
    "test_df['bank_type']  = test_df['card3'].astype(str)+'_'+test_df['card5'].astype(str)\n",
    "\n",
    "train_df['address_match'] = train_df['bank_type'].astype(str)+'_'+train_df['addr2'].astype(str)\n",
    "test_df['address_match']  = test_df['bank_type'].astype(str)+'_'+test_df['addr2'].astype(str)\n",
    "\n",
    "for col in ['address_match','bank_type']:\n",
    "    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n",
    "    temp_df[col] = np.where(temp_df[col].str.contains('nan'), np.nan, temp_df[col])\n",
    "    temp_df = temp_df.dropna()\n",
    "    fq_encode = temp_df[col].value_counts().to_dict()   \n",
    "    train_df[col] = train_df[col].map(fq_encode)\n",
    "    test_df[col]  = test_df[col].map(fq_encode)\n",
    "\n",
    "train_df['address_match'] = train_df['address_match']/train_df['bank_type'] \n",
    "test_df['address_match']  = test_df['address_match']/test_df['bank_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_fe4.csv',index=False)\n",
    "test_df.to_csv('test_fe4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train_total.csv')\n",
    "test=pd.read_csv('test_total.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = train.columns\n",
    "train_df_cols = train_df.columns\n",
    "\n",
    "train_df_not_train = train_df_cols.difference(train_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'C10_fq_enc', u'C11_fq_enc', u'C12_fq_enc', u'C13_fq_enc',\n",
       "       u'C14_fq_enc', u'C1_fq_enc', u'C2_fq_enc', u'C3', u'C3_fq_enc',\n",
       "       u'C4_fq_enc', u'C5_fq_enc', u'C6_fq_enc', u'C7_fq_enc', u'C8_fq_enc',\n",
       "       u'C9_fq_enc', u'D1_fq_enc', u'D2_fq_enc', u'D3_fq_enc', u'D4_fq_enc',\n",
       "       u'D5_fq_enc', u'D6_fq_enc', u'D7_fq_enc', u'D8_fq_enc', u'D9_fq_enc',\n",
       "       u'M4_target_mean', u'P_emaildomain_fq_enc', u'ProductCD_target_mean',\n",
       "       u'R_emaildomain_fq_enc', u'TransactionAmt_check', u'TransactionID',\n",
       "       u'V102', u'V103', u'V104', u'V105', u'V106', u'V108', u'V109', u'V110',\n",
       "       u'V111', u'V112', u'V113', u'V114', u'V115', u'V116', u'V117', u'V118',\n",
       "       u'V119', u'V120', u'V121', u'V122', u'V123', u'V124', u'V125', u'V133',\n",
       "       u'V134', u'V135', u'V136', u'V137', u'V284', u'V286', u'V293', u'V295',\n",
       "       u'V297', u'V299', u'V300', u'V301', u'V305', u'V309', u'V311', u'V316',\n",
       "       u'V318', u'V319', u'V320', u'V321', u'addr1_fq_enc', u'addr2_fq_enc',\n",
       "       u'address_match', u'bank_type', u'card1_TransactionAmt_mean',\n",
       "       u'card1_TransactionAmt_std', u'card1_fq_enc',\n",
       "       u'card2_TransactionAmt_mean', u'card2_TransactionAmt_std',\n",
       "       u'card2_fq_enc', u'card3_TransactionAmt_mean',\n",
       "       u'card3_TransactionAmt_std', u'card3_fq_enc',\n",
       "       u'card5_TransactionAmt_mean', u'card5_TransactionAmt_std',\n",
       "       u'card5_fq_enc', u'dist1_fq_enc', u'dist2_fq_enc', u'uid', u'uid2',\n",
       "       u'uid2_TransactionAmt_mean', u'uid2_TransactionAmt_std',\n",
       "       u'uid_TransactionAmt_mean', u'uid_TransactionAmt_std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_not_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col=list(train_df_not_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[new_col]=train_df[new_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[new_col]=test_df[new_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_total4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test_total4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
